{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "with open (\"../style.css\", \"r\") as file:\n",
    "    css = file.read()\n",
    "HTML(css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection  Using a Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of creating a spam detector using a Support Vector Machine is split up into five steps.\n",
    "\n",
    "  - Create a set of the most common words occurring in spam and ham (i.e. non-spam) emails.\n",
    "  - Transform every mail into a <em style=\"color:blue\">frequency vector</em>: For every word in the set of most common words, \n",
    "    the frequency vector stores the frequency of this word in the respective mail.\n",
    "  - For every word in the list of most common word, compute the <em style=\"color:blue\">inverse document frequency</em>.\n",
    "  - Compute the <em style=\"color:blue\">feature matrix</em> by transforming the frequency vectors into vectors that contain the product of the \n",
    "    <em style=\"color:blue\">term frequency</em> with the <em style=\"color:blue\">inverse document frequency</em>.\n",
    "  - Train and test an SVM using this <em style=\"color:blue\">feature matrix</em>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the Set of Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory \n",
    "https://github.com/karlstroetmann/Artificial-Intelligence/tree/master/Python/EmailData\n",
    "contains 960 emails that are divided into four subdirectories:\n",
    "\n",
    "  - `spam-train` contains 350 spam emails for training,\n",
    "  - `ham-train`  contains 350 non-spam emails for training,\n",
    "  - `spam-test`  contains 130 spam emails for testing,\n",
    "  - `ham-test`   contains 130 non-spam emails for testing.\n",
    "\n",
    "I have found this data on the page \n",
    "http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html provided by Andrew Ng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare some variables so that this notebook can be adapted to other data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dir_train = 'EmailData/spam-train/'\n",
    "ham__dir_train = 'EmailData/ham-train/'\n",
    "spam_dir_test  = 'EmailData/spam-test/'\n",
    "ham__dir_test  = 'EmailData/ham-test/'\n",
    "Directories    = [spam_dir_train, ham__dir_train, spam_dir_test, ham__dir_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{get_word_set}(\\texttt{fn})$ takes a filename $\\texttt{fn}$ as its argument.  It reads the file and returns a `set` of all words that are found in this file.  The words are transformed to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_set(fn):\n",
    "    with open(fn) as file:\n",
    "        text = file.read()\n",
    "        text = text.lower()\n",
    "        return set(re.findall(r\"[\\w']+\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `read_all_files` reads all files contained in those directories that are stored in the list `Directories`. \n",
    "It returns a `Counter`.  For every word $w$ this counter contains the number of files that contain $w$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_files():\n",
    "    Words = Counter()\n",
    "    for directory in Directories:\n",
    "        for file_name in os.listdir(directory):\n",
    "            Words.update(get_words_set(directory + file_name))\n",
    "    return Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Common_Words` is a `numpy` array of the 2500 most common words found in all of our emails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M            = 2500             # number of the most common words to use\n",
    "Word_Counter = read_all_files()\n",
    "Common_Words = np.array(list({ w for w, _ in Word_Counter.most_common(M) }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Common_Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Transform Files into Frequency Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Index_Dict` is a dictionary that maps from the most common words to their index in the array `Common_Words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index_Dict = { w: i for i, w in enumerate(Common_Words) }\n",
    "Index_Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{transform_to_vector}(L)$ takes a list of words $L$ and transforms this list into a vector $\\mathbf{v}$.  If \n",
    "$\\texttt{CommonWords}[i] = w$, then $\\mathbf{v}[i]$ specifies the number of times that $w$ occurs in $L$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_vector(L):\n",
    "    Result = np.zeros((len(Common_Words, )))\n",
    "    for w in L:\n",
    "        if w in Index_Dict:\n",
    "            Result[Index_Dict[w]] += 1\n",
    "    return Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{get_word_vector}(fn)$ takes a filename `fn`, reads the specified file and transforms it into a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(fn):\n",
    "    with open(fn) as file:\n",
    "        text = file.read()\n",
    "        text = text.lower()\n",
    "        return transform_to_vector(re.findall(r\"[\\w']+\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute the Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing, the notion <em style='color:blue;'>term</em> is used as a synonym for <em style='color:blue;'>word</em>.\n",
    "Given a term $t$ and a document $d$, the <em style='color:blue;'>term frequency</em> $\\texttt{tf}(t, d)$ is defined as\n",
    "$$ \\texttt{tf}(t, d) = \\frac{d.\\texttt{count}(t)}{\\texttt{len}(d)}, $$\n",
    "where $d.\\texttt{count}(t)$ counts the number of times $t$ appears in $d$ and $\\texttt{len}(d)$ is the length of the list representing $d$.\n",
    "\n",
    "A <em style='color:blue;'>corpus</em> is a set of documents.  Given a term $t$ and a corpus $\\mathcal{C}$, the <em style='color:blue;'>inverse document frequency</em> \n",
    "$\\texttt{idf}(t,\\mathcal{C})$ is defined as\n",
    "$$ \\texttt{idf}(t,\\mathcal{C}) = \\ln\\left(\\frac{\\texttt{card}(\\mathcal{C}) + 1}{\\texttt{card}\\bigl(\\{ d \\in \\mathcal{C} \\mid t \\in d \\}\\bigr) + 1}\\right). $$\n",
    "The addition of $1$ in both nominator and denominator is called <em style=\"color:blue;\">Laplace smoothing</em>.  This is necessary to prevent a **division by zero** error \n",
    "for those terms $t$ that do not occur in the list `Common_Words`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compute the Feature Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $\\texttt{feature_matrix}(\\texttt{spam_dir}, \\texttt{ham_dir})$ takes two directories that contain spam and ham, respectively.\n",
    "It computes a matrix $X$ and a vector $Y$, where $X$ is the feature matrix and for\n",
    "every row $r$ of the feature matrix, $Y[r]$ is 1 if the mail is ham and 0 if it's spam.\n",
    "\n",
    "The way $X$ is computed is quite inefficient, it would have been better to initialize $X$ as a matrix with the shape $(N,M)$, where $N$ is the number of mails and $M$ is the number of common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix(spam_dir, ham_dir):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for fn in os.listdir(spam_dir):\n",
    "        X.append(get_word_vector(spam_dir + fn))\n",
    "        Y.append(0)\n",
    "    for fn in os.listdir(ham_dir):    \n",
    "        X.append(get_word_vector(ham_dir + fn))\n",
    "        Y.append(+1)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the training set into a feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, Y_train = feature_matrix(spam_dir_train, ham__dir_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, the feature matrix contains only the term frequencies.  Next we multiply with the inverse document frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, _ = X_train.shape\n",
    "IDF  = {}\n",
    "for w, i in Index_Dict.items():\n",
    "    IDF[w] = np.log((N + 1) / (Word_Counter[w] + 1))\n",
    "    X_train[:, i] = X_train[:, i] * IDF[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the feature matrix for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = feature_matrix(spam_dir_test, ham__dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in Index_Dict.items():\n",
    "    X_test[:, i] = X_test[:, i] * IDF[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train and Test a Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an SVM and compute the accuracy on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = svm.SVC(kernel='linear', C=100000)\n",
    "M.fit  (X_train, Y_train)\n",
    "M.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.score(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
