{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "with open (\"../style.css\", \"r\") as file:\n",
    "    css = file.read()\n",
    "HTML(css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we want to investigate is stored in the file `'exam-iq.csv'`.  The first column of this file is an integer from the set $\\{0,1\\}$.  The number is $0$ if the corresponding student has failed the exam and is $1$ otherwise.  The second column is a floating point number that lists the number of hours that the student has studied.  The third column is an integer value specifying the IQ of the student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat exam-iq.csv || type exam-iq.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is read with the library `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExamDF = pd.read_csv('exam-iq.csv')\n",
    "ExamDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the data from the data frame and convert it into `NumPy` arrays.  Furthermore, the pass/fail results are converted into floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ExamDF[['Hours','IQ']])\n",
    "Y = np.array(ExamDF['Pass'], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proceed, we will plot the data points using a scatter plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the <bf style=\"color:red;\">losers</bf> in a different color, we split the data frame `ExamDF` into winners and losers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select both winners and losers by indexing with Boolean arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pass = X[Y == 1.0]\n",
    "X_fail = X[Y == 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('Pass/Fail vs. Hours of Study and IQ')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=85.0, c='k')\n",
    "plt.xlabel('Hours of Study')\n",
    "plt.ylabel('IQ')\n",
    "plt.xticks(np.arange(0.0, 6.0, step=0.25))\n",
    "plt.yticks(np.arange(85, 126, step=2.0))\n",
    "plt.scatter(X_pass[:,0], X_pass[:,1], color='b') # plot student who passed in blue\n",
    "plt.scatter(X_fail[:,0], X_fail[:,1], color='r') # plot the losers red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one student who failed although he has an IQ of 125 and he did study for $3.5$ hours.  Maybe he was still drunk when he had to write the exam.  The student with an IQ of 104 who did pass while only studying for $2$ hours might just have gotten lucky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the module `linear_model` from SciKit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built a *logistic regression* model.   The first parameter `C` is the so called *regularization* parameter.  If we set it to a high value, then we do not regularize.  The second parameter `tol` is the *tolerance*.  It specifies when gradient descent should stop.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = lm.LogisticRegression(C=100_000, tol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the model with the data we have.  The parameter `C` is a *regularization* parameter.  Setting it to a high values avoids any regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "M.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the parameters that we have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϑ0     = M.intercept_[0]\n",
    "ϑ1, ϑ2 = M.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the model we have learned, the probability $P(h,q)$ that a student, who has learned for $h$ hours and has an IQ of $q$, will pass the exam, is given as\n",
    "$$ P(h, q) = S(\\vartheta_0 + \\vartheta_1 \\cdot h + \\vartheta_2 \\cdot q) $$\n",
    "In general, we expect her to pass the exam if\n",
    "$$ \\vartheta_0 + \\vartheta_1 \\cdot h + \\vartheta_2 \\cdot q \\geq 0. $$\n",
    "This can be rewritten as follows:\n",
    "$$  q \\geq -\\frac{\\vartheta_0 + \\vartheta_1 \\cdot h}{\\vartheta_2}. $$\n",
    "Let us plot this borderline $h \\mapsto -\\frac{\\vartheta_0 + \\vartheta_1 \\cdot h}{\\vartheta_2}$ together with the data.\n",
    "This line is also known as the *decision boundary*: Every student whose features are below the decision boundary is predicted to fail the exam, if the features are above the decision boundary, the student is expected to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('Pass/Fail vs. Hours of Study and IQ')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=85.0, c='k')\n",
    "plt.xlabel('Hours of Study')\n",
    "plt.ylabel('IQ')\n",
    "plt.xticks(np.arange(0.0, 6.0, step=0.25))\n",
    "plt.yticks(np.arange(85, 126, step=2.0))\n",
    "plt.scatter(X_pass[:,0], X_pass[:,1], color='blue') # plot student who passed in blue\n",
    "plt.scatter(X_fail[:,0], X_fail[:,1], color='red')  # plot the losers red\n",
    "H = np.arange(2.15, 3.5, 0.05)\n",
    "P = -(ϑ0 + ϑ1 * H)/ϑ2\n",
    "plt.plot(H, P, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that three students are misclassified, but one of them is only misclassified by a small margin \n",
    "as the data points are very close to the green borderline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors   = np.sum(np.abs(Y - M.predict(X)))\n",
    "accuracy = (len(Y) - errors) / len(Y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to predict $85\\%$ of the results correctly.  Instead of computing the accuracy manually we could have used the method `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.score(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that we did not predict the correct result for the student who learned for $2.25$ hours and who has an IQ of $120$ is due to the *outlier* that we have in our data set.  Fortunately, the outlier happens to be the last student in the dataset.  We will remove this student when training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.fit(X[:-1], Y[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only one student is misclassified and hence the accuracy goes up to nearly $95\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.score(X[:-1], Y[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the decision boundary, we have to extract the coefficients of the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϑ0     = M.intercept_[0]\n",
    "ϑ1, ϑ2 = M.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style='darkgrid')\n",
    "plt.title('Pass/Fail vs. Hours of Study and IQ')\n",
    "plt.axvline(x=0.0, c='k')\n",
    "plt.axhline(y=85.0, c='k')\n",
    "plt.xlabel('Hours of Study')\n",
    "plt.ylabel('IQ')\n",
    "plt.xticks(np.arange(0.0, 6.0, step=0.25))\n",
    "plt.yticks(np.arange(85, 126, step=2.0))\n",
    "plt.scatter(X_pass[:,0], X_pass[:,1], color='blue') # plot student who passed in blue\n",
    "plt.scatter(X_fail[:,0], X_fail[:,1], color='red')  # plot the losers red\n",
    "H = np.arange(1.35, 3.5, 0.05)\n",
    "P = -(ϑ0 + ϑ1 * H)/ϑ2\n",
    "plt.plot(H, P, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, logistic regression is able to predict all but two of the results correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
